---
title: "LLM Inference"
weight: 3
bookCollapseSection: false
---

# Running LLMs on AMD GPUs

Guides for running large language models locally on AMD GPUs.

## Tools Covered

- **llama.cpp** - Efficient CPU/GPU inference with GGUF models
- **vLLM** - High-throughput serving with ROCm support
- **Ollama** - Easy local LLM deployment
- **text-generation-inference** - HuggingFace's inference server

## Why AMD GPUs for LLMs?

AMD GPUs offer competitive performance for LLM inference, especially with:
- Large VRAM options (24GB on RX 7900 XTX)
- Good price/performance ratio
- Growing ROCm ecosystem support
